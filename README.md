# ğŸŒ¿ Plant Identifier - Plant Image Classification with Deep Learning

This project trains a convolutional neural network to classify plant species using online images sourced from a CSV file. It follows a clean modular structure for data preparation, training, and model evaluation.

## Setup Instructions

### 1. Environment

```bash
# Create a new conda environment named 'plants'
conda create -n plants python=3.13

# Activate the environment
conda activate plants
```

### 2. Install Dependencies

```bash
# Path of the folder to run
cd "C:\Users\angyp\Documents\anaconda_projects\Fotografia de Plantas\Mod" 

# Install required packages
pip install -r requirements.txt
```

### 3. Launch Jupyter Notebook

```bash
# Start Jupyter Notebook
jupyter notebook
```

Then open `replic.ipynb` in your browser.

## ğŸ“ Projects Structure

```bash

MOD/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â”œâ”€â”€ Dataset_plants/        # Original GBIF TXT/XML dataset
â”‚   â”‚   â”‚   â”œâ”€â”€ dataset/
â”‚   â”‚   â”‚   â”‚    â””â”€â”€ ...           #  XML dataset
â”‚   â”‚   â”‚   â”œâ”€â”€ `citation.txt`
â”‚   â”‚   â”‚   â”œâ”€â”€ `meta.xml`
â”‚   â”‚   â”‚   â”œâ”€â”€ `metadat.xml`
â”‚   â”‚   â”‚   â”œâ”€â”€ `multimedia.txt`
â”‚   â”‚   â”‚   â”œâ”€â”€ `occurrence.txt`
â”‚   â”‚   â”‚   â”œâ”€â”€ `rights.txt`
â”‚   â”‚   â”‚   â””â”€â”€ `verbatim.txt`
â”‚   â”‚   â””â”€â”€ `merged.csv`           # GBIF merged dataset with columns specifics
â”‚   â””â”€â”€ `url_labels.csv`           # Cleaned dataset with URLs and label IDs
â”‚
â”œâ”€â”€ helpers/
â”‚   â”œâ”€â”€ _pycache_/
â”‚   â”œâ”€â”€ `dataset_local.py`         # Class LocalPlantDataset for local images
â”‚   â””â”€â”€ `train_utils.py`           # Training function with metrics/logging
â”‚
â”œâ”€â”€ models/                        # Trained PyTorch models are saved here
â”‚   â”œâ”€â”€ `alexnet.pth`
â”‚   â””â”€â”€ `efficientnet_b3.pth`
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ `download_dataset.ipynb`   # Download the dataset of ``url_labels.csv``
â”‚   â”œâ”€â”€ `preprocess_data.ipynb`    # Cleans `merged.csv` into `url_labels.csv`
â”‚   â”œâ”€â”€ `testing.ipynb`            # Models with random images are tested
â”‚   â”œâ”€â”€ `train_alexnet.ipynb`      # Loads data, trains AlexNet and plots metrics
â”‚   â””â”€â”€ `train_efficientnet.ipynb` # Loads data, trains EfficientNet and plots metrics
â”‚
â”œâ”€â”€ `README.md`                    # Project documentation
â””â”€â”€ `requirements.txt`             # Python dependencies
```

## How to train

### 1. Preprocess the dataset and download

```bash
# Run
notebooks/preprocess_data.ipynb
```

It will create `url_labels.csv` with:

- url: valid image URL (from GBIF identifier or references)
- scientificName
- label_id: integer-encoded label

```bash
# Run
notebooks/download_dataset
```

Then it will download the dataset. Once the dataset is downloaded, folders containing fewer than 100 images will be removed, and only those with at least 100 images will be kept.

### 2. Train Models

#### 2.1. AlexNet

```bash
# Run
notebooks/train_alexnet.ipynb
```

- Loads local images from data/images/
- Trains AlexNet with accuracy and loss logs per epoch
- Saves model to `models/alexnet.pth`

#### 2.2. EfficientNetV2-S

```bash
# Run
notebooks/train_efficientnet.ipynb
```

- Uses the same dataset structure
- Trains EfficientNetV2-S using torchvision.models.efficientnet_v2_s
- Saves model to `models/efficientnet_b3.pth`

### 3. Evaluate

Accuracy and loss graphs are shown automatically after each training.

## Requirements

```bash
# Install dependencies:
pip install -r requirements.txt
```

## Note

- Models can be extended easily with ResNet, VGG, etc.
- EfficientNet tends to perform better on small datasets with fewer parameters.
- Recommended: Use a GPU to accelerate training.
- Sources who maybe would be useful [Deep-Plant GitHub Repository](https://github.com/cs-chan/Deep-Plant)
